{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ddc4f4-f7bf-4e0e-b5a5-5abd8a008b21",
   "metadata": {},
   "source": [
    "# Corrective RAG (CRAG) using local LLMs\n",
    "\n",
    "[Corrective-RAG (CRAG)](https://arxiv.org/abs/2401.15884) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents. \n",
    "\n",
    "* If *any* documents are irrelevant, we'll supplement retrieval with web search. \n",
    "* We'll skip the knowledge refinement, but this can be added back as a node if desired. \n",
    "* I'll use [Tavily Search](https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/) for web search.\n",
    "\n",
    "![alternative text](diagrams:diagram.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4302f-09d9-4d2a-a18d-a6fd23704850",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "use [Ollama](https://ollama.ai/) to access a local LLM:\n",
    "\n",
    "* Download [Ollama app](https://ollama.ai/).\n",
    "* Pull your model of choice, e.g.: `ollama pull llama3`\n",
    "\n",
    "use [Tavily](https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/) for web search.\n",
    "\n",
    "use a vectorstore with [Nomic local embeddings](https://blog.nomic.ai/posts/nomic-embed-text-v1) or, optionally, OpenAI embeddings.\n",
    "\n",
    "use [LangSmith](https://docs.smith.langchain.com/) for tracing and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a660963-bd3d-4c87-b2e4-b6e432055211",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_community tiktoken langchainhub scikit-learn langchain langgraph tavily-python  nomic[local] langchain-nomic langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68316ba0-854b-41e1-9af5-1f9e965946e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \".........\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be68860-dded-481e-9fc7-a5042bf92c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding (optional)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"........\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248ab88-2b97-41eb-8dbb-4ea65525ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracing and testing (optional)\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"........\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"corrective-rag-agent-testing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059c3a3-7f01-4d46-8289-fde4c1b4155f",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "You can select from [Ollama LLMs](https://ollama.com/library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4db331-c4d0-4c7c-a9a5-0bebc8a89c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3\"\n",
    "model_tested = \"llama3-8b\"\n",
    "metadata = f\"CRAG, {model_tested}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b6eed-3b3f-44b5-a34a-4ade1e94caf0",
   "metadata": {},
   "source": [
    "### Index\n",
    "\n",
    "Let's index 3 blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b789b-475b-4e1b-9c66-03504c837830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings  # local\n",
    "from langchain_openai import OpenAIEmbeddings  # api\n",
    "\n",
    "# List of URLs to load documents from\n",
    "urls = [\n",
    "    \"https://towardsdatascience.com/transformers-141e32e69591\",\n",
    "    \"https://aws.amazon.com/what-is/prompt-engineering/\",\n",
    "    \"https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns\",\n",
    "]\n",
    "\n",
    "# Load documents from the URLs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Initialize a text splitter with specified chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Embedding\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# Add the document chunks to the \"vector store\"\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe7fd10a-f64a-48de-a116-6d5890def1af",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e75c029-6c10-47c7-871c-1f4932b25309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': '1'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a teacher grading a quiz. You will be given: \n",
    "    1/ a QUESTION\n",
    "    2/ A FACT provided by the student\n",
    "    \n",
    "    You are grading RELEVANCE RECALL:\n",
    "    A score of 1 means that ANY of the statements in the FACT are relevant to the QUESTION. \n",
    "    A score of 0 means that NONE of the statements in the FACT are relevant to the QUESTION. \n",
    "    1 is the highest (best) score. 0 is the lowest score you can give. \n",
    "    \n",
    "    Explain your reasoning in a step-by-step manner. Ensure your reasoning and conclusion are correct. \n",
    "    \n",
    "    Avoid simply stating the correct answer at the outset.\n",
    "    \n",
    "    Question: {question} \\n\n",
    "    Fact: \\n\\n {documents} \\n\\n\n",
    "    \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"Convolutional neural network\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"documents\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad03302-bd93-43fc-949e-af51a3298cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    \n",
    "    Use the following documents to answer the question. \n",
    "    \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Documents: {documents} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36a2f36-bc5f-408d-a5e8-3fa203c233f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3421cf0-9067-43fe-8681-0d3189d15dd3",
   "metadata": {},
   "source": [
    "### Graph \n",
    "\n",
    "Here we'll explicitly define the majority of the control flow, only using an LLM to define a single branch point following grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10028794-2fbc-43f9-aa4c-7fe3abd69c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    search: str\n",
    "    documents: List[str]\n",
    "    steps: List[str]\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"retrieve_documents\")\n",
    "    return {\"documents\": documents, \"question\": question, \"steps\": steps}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"generate_answer\")\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "        \"steps\": steps,\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"grade_document_retrieval\")\n",
    "    filtered_docs = []\n",
    "    search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"documents\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            search = \"Yes\"\n",
    "            continue\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"search\": search,\n",
    "        \"steps\": steps,\n",
    "    }\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    steps = state[\"steps\"]\n",
    "    steps.append(\"web_search\")\n",
    "    web_results = web_search_tool.invoke({\"query\": question})\n",
    "    documents.extend(\n",
    "        [\n",
    "            Document(page_content=d[\"content\"], metadata={\"url\": d[\"url\"]})\n",
    "            for d in web_results\n",
    "        ]\n",
    "    )\n",
    "    return {\"documents\": documents, \"question\": question, \"steps\": steps}\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    search = state[\"search\"]\n",
    "    if search == \"Yes\":\n",
    "        return \"search\"\n",
    "    else:\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"search\": \"web_search\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "custom_graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2e04a",
   "metadata": {},
   "source": [
    "![alternative text](workflow.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
